Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4265428,11.289479,2077.5833333333335,-2.4164983220398426,-2.4164983220398426,4.1008835,0.024113819,0.00028457626,0.19485873,0.0047434513,1.0
100000,1.4438267,9.337014,2253.318181818182,-2.753907264379615,-2.753907264379615,2.4270515,0.022303563,0.00025684276,0.18561423,0.00428215,1.0
150000,1.4593192,6.764077,1927.8,-2.4145999887399374,-2.4145999887399374,1.3218875,0.02256865,0.00022602761,0.17534252,0.003769592,1.0
200000,1.4667249,4.7846847,2150.4583333333335,-2.6876383240548116,-2.6876383240548116,0.6351482,0.021885946,0.00019517427,0.1650581,0.003256398,1.0
250000,1.4746665,3.0276675,1924.0,-2.423487680044043,-2.423487680044043,0.26056692,0.022581179,0.00016434079,0.15478024,0.0027435338,1.0
300000,1.482951,2.487052,2110.9565217391305,-2.6494191131433067,-2.6494191131433067,0.15883368,0.023263153,0.00013354831,0.14451608,0.002231352,1.0
