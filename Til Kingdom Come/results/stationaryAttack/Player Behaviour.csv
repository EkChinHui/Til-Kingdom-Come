Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4177573,-0.66774803,307.05732484076435,14.87584516083359,14.87584516083359,2.6566875,0.023139575,0.0002846085,0.1948695,0.0047439886,1.0
100000,1.4265959,0.41010454,232.926267281106,22.618944512534252,22.618944512534252,2.3975894,0.022685388,0.00025687166,0.18562388,0.004282631,1.0
150000,1.4296379,1.5599397,196.29019607843136,24.698616394575904,24.698616394575904,2.9074082,0.019836865,0.00022604983,0.17534992,0.003769961,1.0
200000,1.4296001,2.8137834,183.8111111111111,26.66830168388508,26.66830168388508,3.2396588,0.023115428,0.00019521895,0.16507296,0.003257141,1.0
250000,1.4290096,3.9301097,174.02447552447552,27.57462931075296,27.57462931075296,3.350734,0.022563249,0.00016439415,0.15479803,0.0027444218,1.0
300000,1.4250268,4.873083,171.44636678200692,28.019029817572928,28.019029817572928,3.489973,0.023903832,0.00013354796,0.14451596,0.0022313467,1.0
350000,1.4210823,5.7518053,165.98338870431894,28.268102938748672,28.268102938748672,3.4135947,0.024191614,0.00010271289,0.1342376,0.0017184562,1.0
400000,1.4187756,6.480726,167.58862876254182,28.072771232800196,28.072771232800196,3.457925,0.026203724,7.497414e-05,0.12499135,0.0012570682,1.0
450000,1.4152232,7.106113,168.3924914675768,28.549532411455296,28.549532411455296,3.6433792,0.022094097,4.7258007e-05,0.11575265,0.00079605664,1.0
500000,1.4134222,7.608615,167.5627118644068,30.044341829267598,30.044341829267598,4.0311613,0.021439167,1.6430862e-05,0.10547693,0.00028329837,1.0
