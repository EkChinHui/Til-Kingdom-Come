Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4267501,0.36930707,319.8466666666667,13.631166411091884,13.631166411091884,1.903312,0.023572773,0.00028457073,0.1948569,0.0047433595,1.0
100000,1.4390981,1.2565247,330.0326797385621,13.927290887809267,13.927290887809267,1.1648172,0.021155125,0.00025683962,0.18561319,0.0042820987,1.0
150000,1.4455099,1.9669302,348.1468531468532,12.1111185359736,12.1111185359736,1.0646114,0.02217327,0.00022604693,0.17534897,0.0037699125,1.0
200000,1.4467897,2.4783318,324.6168831168831,13.670835226798564,13.670835226798564,1.0751841,0.024812546,0.00019525194,0.16508397,0.0032576893,1.0
250000,1.4473096,2.825475,325.9078947368421,14.572959540302262,14.572959540302262,1.1593119,0.021077454,0.00016444757,0.15481584,0.0027453105,1.0
300000,1.4481655,3.2235494,317.12738853503186,14.620271651418346,14.620271651418346,1.1765009,0.023430863,0.00013361574,0.14453857,0.002232474,1.0
350000,1.4453529,3.2558358,333.76027397260276,12.723577946985829,12.723577946985829,1.200828,0.02709887,0.000102801794,0.13426724,0.0017199351,1.0
400000,1.4437962,3.0359018,320.8050314465409,14.739050840056917,14.739050840056917,1.2517446,0.019598924,7.502889e-05,0.1250096,0.0012579792,1.0
450000,1.4446949,3.2385285,338.265306122449,13.036956213997538,13.036956213997538,1.2308812,0.024378007,4.728957e-05,0.115763165,0.0007965816,1.0
500000,1.4448228,3.150244,320.14649681528664,14.124451896472342,14.124451896472342,1.3064929,0.022926709,1.6469976e-05,0.105489954,0.000283949,1.0
